{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Domain-Specific Assistant: Agriculture QA via LLM Fine-Tuning\n",
        "\n",
        "This notebook fine-tunes **TinyLlama-1.1B-Chat** on the **sowmya14/agriculture_QA** dataset using **LoRA (PEFT)** on Google Colab. It includes data preprocessing, training, evaluation (BLEU, ROUGE, perplexity), and a Gradio UI.\n",
        "\n",
        "**Model**: [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)  \n",
        "**Dataset**: [sowmya14/agriculture_QA](https://huggingface.co/datasets/sowmya14/agriculture_QA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install dependencies\n",
        "\n",
        "Run this cell first (Colab: Runtime â†’ Change runtime type â†’ GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "afaceff1",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%uv pip install -q transformers datasets peft accelerate bitsandbytes evaluate nltk gradio pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7fe5d694",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import re\n",
        "\n",
        "# Config (edit for experiments)\n",
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "DATASET_ID = \"sowmya14/agriculture_QA\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "OUTPUT_DIR = \"./agriculture_assistant_lora\"\n",
        "USE_4BIT = True  # set False if you have enough GPU memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and inspect dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35aee8fe942f4d3d939f48aa869a7091",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.csv: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfb61635ac2348bc8402d05136c5c1a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/999 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splits: ['train']\n",
            "Columns: ['questions', 'answers']\n",
            "Num examples: 999\n",
            "Sample row: {'questions': 'asking about the control measure for aphid infestation in mustard crops', 'answers': 'suggested him to spray rogor@2ml/lit.at evening time.'}\n"
          ]
        }
      ],
      "source": [
        "ds = load_dataset(DATASET_ID)\n",
        "print(\"Splits:\", list(ds.keys()))\n",
        "split = \"train\" if \"train\" in ds else list(ds.keys())[0]\n",
        "d = ds[split]\n",
        "print(\"Columns:\", d.column_names)\n",
        "print(\"Num examples:\", len(d))\n",
        "print(\"Sample row:\", d[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preprocessing: normalize and format as instructionâ€“response\n",
        "\n",
        "We map dataset columns to a standard `instruction` / `response` format, normalize text, and keep sequences within the model context length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using question column: 'questions', answer column: 'answers'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbce2dbefb63470bb21092b36f37eed1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/999 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3817e87788d458b9675ded9e531ee28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/999 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted examples: 999\n",
            "Sample: {'instruction': 'You are an agriculture assistant. Answer the following question.\\n\\nQuestion: asking about the control measure for aphid infestation in mustard crops', 'response': 'suggested him to spray rogor@2ml/lit.at evening time.'}\n"
          ]
        }
      ],
      "source": [
        "def normalize_text(text):\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def get_qa_columns(dataset):\n",
        "    cols = dataset.column_names\n",
        "    q_col = None\n",
        "    a_col = None\n",
        "    for c in cols:\n",
        "        lower = c.lower()\n",
        "        if lower in (\"question\", \"questions\", \"input\", \"query\"):\n",
        "            q_col = c\n",
        "        if lower in (\"answer\", \"answers\", \"output\", \"response\"):\n",
        "            a_col = c\n",
        "    if q_col is None:\n",
        "        q_col = cols[0]\n",
        "    if a_col is None:\n",
        "        a_col = cols[1] if len(cols) > 1 else cols[0]\n",
        "    return q_col, a_col\n",
        "\n",
        "def format_instruction_response(example, q_col, a_col):\n",
        "    q = normalize_text(example.get(q_col, \"\"))\n",
        "    a = normalize_text(example.get(a_col, \"\"))\n",
        "    instruction = f\"You are an agriculture assistant. Answer the following question.\\n\\nQuestion: {q}\"\n",
        "    return {\"instruction\": instruction, \"response\": a}\n",
        "\n",
        "q_col, a_col = get_qa_columns(ds[split])\n",
        "print(f\"Using question column: '{q_col}', answer column: '{a_col}'\")\n",
        "\n",
        "def map_to_instruction_response(examples):\n",
        "    out = {\"instruction\": [], \"response\": []}\n",
        "    for i in range(len(examples[q_col])):\n",
        "        ex = {k: v[i] for k, v in examples.items()}\n",
        "        formatted = format_instruction_response(ex, q_col, a_col)\n",
        "        if formatted[\"instruction\"] and formatted[\"response\"]:\n",
        "            out[\"instruction\"].append(formatted[\"instruction\"])\n",
        "            out[\"response\"].append(formatted[\"response\"])\n",
        "    return out\n",
        "\n",
        "ds_qa = ds[split].map(map_to_instruction_response, batched=True, remove_columns=ds[split].column_names)\n",
        "ds_qa = ds_qa.filter(lambda x: len(x[\"instruction\"]) > 0 and len(x[\"response\"]) > 0)\n",
        "print(\"Formatted examples:\", len(ds_qa))\n",
        "print(\"Sample:\", ds_qa[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*(Perplexity is computed in **Section 8** after training.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tokenization and train/validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db04303bbbcc4abbae95c802d46b65aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a77dc961645d46d29f8c7d7079d386ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e6d1c40c23449df8a299611e337a3ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c631854692b47ff9818b399b2642678",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5605911d3fc3459bac51477cbfc98f4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/899 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3990280c4f534241bc45a3aa2d6ab0f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 899 Eval size: 100\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Use chat template if available; otherwise simple concatenation\n",
        "def tokenize_function(examples):\n",
        "    texts = []\n",
        "    for inst, resp in zip(examples[\"instruction\"], examples[\"response\"]):\n",
        "        # TinyLlama chat format: <|system|>...<|user|>...<|assistant|>...\n",
        "        text = f\"<|system|>\\nYou are an agriculture assistant.\\n<|user|>\\n{inst}\\n<|assistant|>\\n{resp}\"\n",
        "        texts.append(text)\n",
        "    out = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    out[\"labels\"] = [list(x) for x in out[\"input_ids\"]]\n",
        "    return out\n",
        "\n",
        "ds_split = ds_qa.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = ds_split[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"response\"])\n",
        "eval_ds = ds_split[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"response\"])\n",
        "train_ds.set_format(\"torch\")\n",
        "eval_ds.set_format(\"torch\")\n",
        "print(\"Train size:\", len(train_ds), \"Eval size:\", len(eval_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load base model and apply LoRA (PEFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce01212115c0425cb04ba2bb2d1bdec7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c95f06b84b8c4a4587990990edf5c2ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bc8a3bc9adc4513a975b32719c11de5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
          ]
        }
      ],
      "source": [
        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "if USE_4BIT:\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        dtype=compute_dtype,\n",
        "    )\n",
        "\n",
        "model = prepare_model_for_kbit_training(model) if USE_4BIT else model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cc024f97",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "/usr/local/lib/python3.12/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [226/226 45:08, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.313200</td>\n",
              "      <td>0.242143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>0.213416</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./agriculture_assistant_lora/tokenizer_config.json',\n",
              " './agriculture_assistant_lora/special_tokens_map.json',\n",
              " './agriculture_assistant_lora/chat_template.jinja',\n",
              " './agriculture_assistant_lora/tokenizer.model',\n",
              " './agriculture_assistant_lora/added_tokens.json',\n",
              " './agriculture_assistant_lora/tokenizer.json')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_ratio=0.05,\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "train_start = time.time()\n",
        "trainer.train()\n",
        "train_elapsed_min = (time.time() - train_start) / 60\n",
        "gpu_mem_gb = round(torch.cuda.max_memory_allocated(0) / 1e9, 2) if torch.cuda.is_available() else None\n",
        "print(f\"Training time: {train_elapsed_min:.1f} min\")\n",
        "if gpu_mem_gb is not None:\n",
        "    print(f\"Max GPU memory: {gpu_mem_gb} GB\")\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation: ROUGE, BLEU, and qualitative check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%uv pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0a4544462e749d2a80e0aacca004fe2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5f50846a45447eb8c350a01bb09a026",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eefb1f3d133d466c9de25295c4a5003b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE: {'rouge1': np.float64(0.13505111638604944), 'rouge2': np.float64(0.039232436785439834), 'rougeL': np.float64(0.12803137749181473), 'rougeLsum': np.float64(0.12903991500775863)}\n",
            "BLEU: {'bleu': 0.0, 'precisions': [0.16981132075471697, 0.04538341158059468, 0.0050933786078098476, 0.0], 'brevity_penalty': 0.877547588776899, 'length_ratio': 0.8844672657252889, 'translation_length': 689, 'reference_length': 779}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:46]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval loss: 0.21341587603092194 | Perplexity: 1.237899356880534\n"
          ]
        }
      ],
      "source": [
        "from evaluate import load as load_metric\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "bleu = load_metric(\"bleu\")\n",
        "\n",
        "def generate_response(model, tokenizer, instruction, max_new_tokens=128):\n",
        "    prompt = f\"<|system|>\\nYou are an agriculture assistant.\\n<|user|>\\n{instruction}\\n<|assistant|>\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "    reply = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    return reply.strip()\n",
        "\n",
        "eval_sample = min(50, len(eval_ds))\n",
        "references = []\n",
        "predictions = []\n",
        "for i in range(eval_sample):\n",
        "    ex = ds_split[\"test\"][i]\n",
        "    ref = ex[\"response\"]\n",
        "    pred = generate_response(model, tokenizer, ex[\"instruction\"])\n",
        "    references.append(ref)\n",
        "    predictions.append(pred)\n",
        "\n",
        "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "print(\"ROUGE:\", rouge_result)\n",
        "\n",
        "# BLEU expects list of strings and list of list of strings\n",
        "refs_bleu = [[r] for r in references]\n",
        "bleu_result = bleu.compute(predictions=predictions, references=refs_bleu)\n",
        "print(\"BLEU:\", bleu_result)\n",
        "\n",
        "# Perplexity = exp(eval_loss)\n",
        "eval_metrics = trainer.evaluate()\n",
        "eval_loss = eval_metrics.get(\"eval_loss\", float(\"nan\"))\n",
        "perplexity = np.exp(eval_loss) if isinstance(eval_loss, (int, float)) else float(\"nan\")\n",
        "print(\"Eval loss:\", eval_loss, \"| Perplexity:\", perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ea1e64",
      "metadata": {},
      "source": [
        "## 9. Experiment table (auto-filled)\n",
        "\n",
        "Run the cell below **after** Section 8 (evaluation). It fills the table from the current run. For multiple experiments, change config (LR, epochs, LoRA r, etc.), re-run from Section 6 (or 7) through Section 8, then run this cell againâ€”new rows are appended to the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da60d64",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-fill experiment table from this run (run after Section 8 evaluation)\n",
        "import pandas as pd\n",
        "\n",
        "# Current run metrics (from training and eval cells)\n",
        "lr = getattr(training_args, \"learning_rate\", 5e-5)\n",
        "batch = getattr(training_args, \"per_device_train_batch_size\", 2)\n",
        "grad_acc = getattr(training_args, \"gradient_accumulation_steps\", 4)\n",
        "epochs = getattr(training_args, \"num_train_epochs\", 3)\n",
        "lora_r = getattr(lora_config, \"r\", 8)\n",
        "val_loss = eval_loss if \"eval_loss\" in dir() else float(\"nan\")\n",
        "rl = rouge_result.get(\"rougeL\") if \"rouge_result\" in dir() else None\n",
        "rouge_l = rl.get(\"fmeasure\", rl) if isinstance(rl, dict) else (rl if rl is not None else float(\"nan\"))\n",
        "bleu_score = bleu_result.get(\"bleu\", float(\"nan\")) if \"bleu_result\" in dir() else float(\"nan\")\n",
        "time_min = round(train_elapsed_min, 1) if \"train_elapsed_min\" in dir() else None\n",
        "gpu_gb = gpu_mem_gb if \"gpu_mem_gb\" in dir() else None\n",
        "\n",
        "# Append to experiment log (persists across runs in this session)\n",
        "if \"experiment_log\" not in globals():\n",
        "    experiment_log = []\n",
        "experiment_log.append({\n",
        "    \"Exp\": len(experiment_log) + 1,\n",
        "    \"LR\": lr,\n",
        "    \"Batch\": f\"{batch} (acc {grad_acc})\",\n",
        "    \"Epochs\": epochs,\n",
        "    \"LoRA r\": lora_r,\n",
        "    \"Val loss\": round(val_loss, 4) if isinstance(val_loss, (int, float)) else \"â€”\",\n",
        "    \"ROUGE-L\": round(rouge_l, 4) if isinstance(rouge_l, (int, float)) else \"â€”\",\n",
        "    \"BLEU\": round(bleu_score, 4) if isinstance(bleu_score, (int, float)) else \"â€”\",\n",
        "    \"Time (min)\": time_min if time_min is not None else \"â€”\",\n",
        "    \"GPU mem (GB)\": gpu_gb if gpu_gb is not None else \"â€”\",\n",
        "    \"Notes\": \"Default\" if len(experiment_log) == 0 else f\"Run {len(experiment_log) + 1}\",\n",
        "})\n",
        "\n",
        "df = pd.DataFrame(experiment_log)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8b. Base vs fine-tuned comparison (for report and demo)\n",
        "\n",
        "The assignment requires comparing the **base pre-trained model** with the **fine-tuned** model. Run the cell below to get responses from both on the same questions. Use this output in your report and demo video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "BASE vs FINE-TUNED (use in report and demo video)\n",
            "================================================================================\n",
            "\n",
            "--- Example 1 ---\n",
            "Question: asking about how to avail kisan credit card loan for sali crop....\n",
            "Reference:   answer is given in details...\n",
            "Base:        Sure, I'd be happy to help you with that.\n",
            "\n",
            "To avail a Kisan Credit Card Loan for Sali Crop, you can follow these steps:\n",
            "\n",
            "1. Check your eligibility: Before applying for a Kisan Cred...\n",
            "Fine-tuned:  suggested to apply for kisan credit card loan for sali crop....\n",
            "\n",
            "--- Example 2 ---\n",
            "Question: asking about source of early ahu rice variety...\n",
            "Reference:   transfer to vet expert...\n",
            "Base:        Yes, I can provide you with information about the source of early ahu rice variety. Early ahu rice variety is a type of rice that has been cultivated in the Philippines for centuri...\n",
            "Fine-tuned:  suggested to use 100 gms of urea and 100 gms of ammonium sulphate in 10 liters of water....\n",
            "\n",
            "--- Example 3 ---\n",
            "Question: asking that he has not got proper friut from his coconut plant...\n",
            "Reference:   profex super 2ml/lit,blitox 2g/lit...\n",
            "Base:        Yes, you are an agriculture assistant. The given text is a response to the question asked by the user. The given text does not mention that the person has not got proper fruit from...\n",
            "Fine-tuned:  suggested to add 1/2 cup of fresh coconut water to the soil....\n",
            "\n",
            "--- Example 4 ---\n",
            "Question: asking about the control measure of flower drop problem in his coconut plant...\n",
            "Reference:   advised, 1.ostocalcium.....20ml/100 birds ..in water for 10 days 2.put small stones in water trough 3. put on red light bulb at night...\n",
            "Base:        Yes, I can answer the question.\n",
            "\n",
            "Question: Asking about the control measure of flower drop problem in his coconut plant\n",
            "\n",
            "Answer: Yes, I can answer the question.\n",
            "\n",
            "Question: Asking a...\n",
            "Fine-tuned:  suggested to apply 100 gms of neem oil on the plant....\n",
            "\n",
            "--- Example 5 ---\n",
            "Question: asking about treatment of low production of milk in cow...\n",
            "Reference:   suggested first to drain-off the excess water from the field to kill the floating larvae and to spray ekalux 25ec@2ml/liter of water....\n",
            "Base:        Yes, I can answer the question.\n",
            "\n",
            "Treatment of low production of milk in cow:\n",
            "\n",
            "1. Cow is fed with low quality feed.\n",
            "2. Cow is not given enough time to graze.\n",
            "3. Cow is not given eno...\n",
            "Fine-tuned:  suggested to use 100 gms of urea in 10 liters of water and to apply it on the udder of the cow....\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Compare base (no LoRA) vs fine-tuned. If OOM, set NUM_COMPARE = 2.\n",
        "NUM_COMPARE = 5\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "if USE_4BIT:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        ),\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "else:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID, device_map=\"auto\", trust_remote_code=True, torch_dtype=compute_dtype\n",
        "    )\n",
        "base_model.eval()\n",
        "\n",
        "compare_instructions = [ds_split[\"test\"][i][\"instruction\"] for i in range(min(NUM_COMPARE, len(ds_split[\"test\"])))]\n",
        "compare_references = [ds_split[\"test\"][i][\"response\"] for i in range(min(NUM_COMPARE, len(ds_split[\"test\"])))]\n",
        "base_responses = [generate_response(base_model, tokenizer, inst) for inst in compare_instructions]\n",
        "finetuned_responses = list(predictions[:NUM_COMPARE])\n",
        "\n",
        "del base_model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BASE vs FINE-TUNED (use in report and demo video)\")\n",
        "print(\"=\" * 80)\n",
        "for i in range(len(compare_instructions)):\n",
        "    q = compare_instructions[i].split(\"Question:\")[-1].strip()[:80]\n",
        "    print(f\"\\n--- Example {i+1} ---\\nQuestion: {q}...\")\n",
        "    print(f\"Reference:   {compare_references[i][:180]}...\")\n",
        "    print(f\"Base:        {base_responses[i][:180]}...\")\n",
        "    print(f\"Fine-tuned:  {finetuned_responses[i][:180]}...\")\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bb36ce3f",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_223/547291445.py:11: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
            "  with gr.Blocks(title=\"Agriculture Assistant\", theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://3218a2782f4b1fb020.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://3218a2782f4b1fb020.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/queueing.py\", line 766, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/route_utils.py\", line 355, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/blocks.py\", line 2163, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/blocks.py\", line 1940, in postprocess_data\n",
            "    prediction_value = block.postprocess(prediction_value)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/components/chatbot.py\", line 704, in postprocess\n",
            "    self._check_format(value)\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/components/chatbot.py\", line 402, in _check_format\n",
            "    raise Error(\n",
            "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/queueing.py\", line 766, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/route_utils.py\", line 355, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/blocks.py\", line 2163, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/blocks.py\", line 1940, in postprocess_data\n",
            "    prediction_value = block.postprocess(prediction_value)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/components/chatbot.py\", line 704, in postprocess\n",
            "    self._check_format(value)\n",
            "  File \"/usr/local/lib/python3.12/site-packages/gradio/components/chatbot.py\", line 402, in _check_format\n",
            "    raise Error(\n",
            "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat(user_input, history=None):\n",
        "    if history is None:\n",
        "        history = []\n",
        "    instruction = f\"You are an agriculture assistant. Answer the following question.\\n\\nQuestion: {user_input}\"\n",
        "    reply = generate_response(model, tokenizer, instruction)\n",
        "    history = history + [\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "        {\"role\": \"assistant\", \"content\": reply},\n",
        "    ]\n",
        "    return history, history\n",
        "\n",
        "with gr.Blocks(title=\"Agriculture Assistant\") as demo:\n",
        "    gr.Markdown(\"\"\"## ðŸŒ¾ Agriculture QA Assistant\\n\\n**Instructions:** Type your agriculture-related question in the box below and click **Submit** (or press Enter). The fine-tuned model will answer. Use **Clear** to start a new conversation. For your demo video, try in-domain questions (e.g. pests, crops, soil) and optionally an out-of-domain question to show the model stays on topic.\\n\\nModel fine-tuned on [sowmya14/agriculture_QA](https://huggingface.co/datasets/sowmya14/agriculture_QA).\"\"\")\n",
        "    chatbot = gr.Chatbot(label=\"Chat\")\n",
        "    msg = gr.Textbox(placeholder=\"e.g. What are the best practices for soil preparation?\", label=\"Your question\")\n",
        "    submit = gr.Button(\"Submit\")\n",
        "    clear = gr.Button(\"Clear\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    def submit_fn(msg, history):\n",
        "        if not msg.strip():\n",
        "            return history, history\n",
        "        _, new_history = chat(msg, history)\n",
        "        return new_history, new_history\n",
        "\n",
        "    submit.click(submit_fn, [msg, state], [chatbot, state])\n",
        "    clear.click(lambda: ([], []), None, [chatbot, state])\n",
        "    msg.submit(submit_fn, [msg, state], [chatbot, state])\n",
        "\n",
        "# In Colab the app appears below; share=True gives a public URL for your demo video.\n",
        "demo.launch(share=True, theme=gr.themes.Soft())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
