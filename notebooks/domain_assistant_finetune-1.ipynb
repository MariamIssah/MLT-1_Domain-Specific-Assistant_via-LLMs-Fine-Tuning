{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Domain-Specific Assistant: Agriculture QA via LLM Fine-Tuning\n",
        "\n",
        "This notebook fine-tunes **TinyLlama-1.1B-Chat** on the **sowmya14/agriculture_QA** dataset using **LoRA (PEFT)** on Google Colab. It includes data preprocessing, training, evaluation (BLEU, ROUGE, perplexity), and a Gradio UI.\n",
        "\n",
        "**Model**: [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)  \n",
        "**Dataset**: [sowmya14/agriculture_QA](https://huggingface.co/datasets/sowmya14/agriculture_QA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install dependencies\n",
        "\n",
        "Run this cell first (Colab: Runtime â†’ Change runtime type â†’ GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~vicorn (C:\\Users\\awini\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~vicorn (C:\\Users\\awini\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~vicorn (C:\\Users\\awini\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes evaluate nltk gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\awini\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import re\n",
        "\n",
        "# Config (edit for experiments)\n",
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "DATASET_ID = \"sowmya14/agriculture_QA\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "OUTPUT_DIR = \"./agriculture_assistant_lora\"\n",
        "USE_4BIT = True  # set False if you have enough GPU memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and inspect dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splits: ['train']\n",
            "Columns: ['questions', 'answers']\n",
            "Num examples: 999\n",
            "Sample row: {'questions': 'asking about the control measure for aphid infestation in mustard crops', 'answers': 'suggested him to spray rogor@2ml/lit.at evening time.'}\n"
          ]
        }
      ],
      "source": [
        "ds = load_dataset(DATASET_ID)\n",
        "print(\"Splits:\", list(ds.keys()))\n",
        "split = \"train\" if \"train\" in ds else list(ds.keys())[0]\n",
        "d = ds[split]\n",
        "print(\"Columns:\", d.column_names)\n",
        "print(\"Num examples:\", len(d))\n",
        "print(\"Sample row:\", d[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preprocessing: normalize and format as instructionâ€“response\n",
        "\n",
        "We map dataset columns to a standard `instruction` / `response` format, normalize text, and keep sequences within the model context length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using question column: 'questions', answer column: 'answers'\n",
            "Formatted examples: 999\n",
            "Sample: {'instruction': 'You are an agriculture assistant. Answer the following question.\\n\\nQuestion: asking about the control measure for aphid infestation in mustard crops', 'response': 'suggested him to spray rogor@2ml/lit.at evening time.'}\n"
          ]
        }
      ],
      "source": [
        "def normalize_text(text):\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def get_qa_columns(dataset):\n",
        "    cols = dataset.column_names\n",
        "    q_col = None\n",
        "    a_col = None\n",
        "    for c in cols:\n",
        "        lower = c.lower()\n",
        "        if lower in (\"question\", \"questions\", \"input\", \"query\"):\n",
        "            q_col = c\n",
        "        if lower in (\"answer\", \"answers\", \"output\", \"response\"):\n",
        "            a_col = c\n",
        "    if q_col is None:\n",
        "        q_col = cols[0]\n",
        "    if a_col is None:\n",
        "        a_col = cols[1] if len(cols) > 1 else cols[0]\n",
        "    return q_col, a_col\n",
        "\n",
        "def format_instruction_response(example, q_col, a_col):\n",
        "    q = normalize_text(example.get(q_col, \"\"))\n",
        "    a = normalize_text(example.get(a_col, \"\"))\n",
        "    instruction = f\"You are an agriculture assistant. Answer the following question.\\n\\nQuestion: {q}\"\n",
        "    return {\"instruction\": instruction, \"response\": a}\n",
        "\n",
        "q_col, a_col = get_qa_columns(ds[split])\n",
        "print(f\"Using question column: '{q_col}', answer column: '{a_col}'\")\n",
        "\n",
        "def map_to_instruction_response(examples):\n",
        "    out = {\"instruction\": [], \"response\": []}\n",
        "    for i in range(len(examples[q_col])):\n",
        "        ex = {k: v[i] for k, v in examples.items()}\n",
        "        formatted = format_instruction_response(ex, q_col, a_col)\n",
        "        if formatted[\"instruction\"] and formatted[\"response\"]:\n",
        "            out[\"instruction\"].append(formatted[\"instruction\"])\n",
        "            out[\"response\"].append(formatted[\"response\"])\n",
        "    return out\n",
        "\n",
        "ds_qa = ds[split].map(map_to_instruction_response, batched=True, remove_columns=ds[split].column_names)\n",
        "ds_qa = ds_qa.filter(lambda x: len(x[\"instruction\"]) > 0 and len(x[\"response\"]) > 0)\n",
        "print(\"Formatted examples:\", len(ds_qa))\n",
        "print(\"Sample:\", ds_qa[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*(Perplexity is computed in **Section 8** after training.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tokenization and train/validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 899 Eval size: 100\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Use chat template if available; otherwise simple concatenation\n",
        "def tokenize_function(examples):\n",
        "    texts = []\n",
        "    for inst, resp in zip(examples[\"instruction\"], examples[\"response\"]):\n",
        "        # TinyLlama chat format: <|system|>...<|user|>...<|assistant|>...\n",
        "        text = f\"<|system|>\\nYou are an agriculture assistant.\\n<|user|>\\n{inst}\\n<|assistant|>\\n{resp}\"\n",
        "        texts.append(text)\n",
        "    out = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    out[\"labels\"] = [list(x) for x in out[\"input_ids\"]]\n",
        "    return out\n",
        "\n",
        "ds_split = ds_qa.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = ds_split[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"response\"])\n",
        "eval_ds = ds_split[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"response\"])\n",
        "train_ds.set_format(\"torch\")\n",
        "eval_ds.set_format(\"torch\")\n",
        "print(\"Train size:\", len(train_ds), \"Eval size:\", len(eval_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load base model and apply LoRA (PEFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [02:23<00:00,  1.40it/s, Materializing param=model.norm.weight]                              \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
          ]
        }
      ],
      "source": [
        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "if USE_4BIT:\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        dtype=compute_dtype,\n",
        "    )\n",
        "\n",
        "model = prepare_model_for_kbit_training(model) if USE_4BIT else model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
            "c:\\Users\\awini\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "c:\\Users\\awini\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\data\\data_collator.py:600: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
            "c:\\Users\\awini\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_ratio=0.05,\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation: ROUGE, BLEU, and qualitative check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluate import load as load_metric\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "bleu = load_metric(\"bleu\")\n",
        "\n",
        "def generate_response(model, tokenizer, instruction, max_new_tokens=128):\n",
        "    prompt = f\"<|system|>\\nYou are an agriculture assistant.\\n<|user|>\\n{instruction}\\n<|assistant|>\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "    reply = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    return reply.strip()\n",
        "\n",
        "eval_sample = min(50, len(eval_ds))\n",
        "references = []\n",
        "predictions = []\n",
        "for i in range(eval_sample):\n",
        "    ex = ds_split[\"test\"][i]\n",
        "    ref = ex[\"response\"]\n",
        "    pred = generate_response(model, tokenizer, ex[\"instruction\"])\n",
        "    references.append(ref)\n",
        "    predictions.append(pred)\n",
        "\n",
        "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "print(\"ROUGE:\", rouge_result)\n",
        "\n",
        "# BLEU expects list of strings and list of list of strings\n",
        "refs_bleu = [[r] for r in references]\n",
        "bleu_result = bleu.compute(predictions=predictions, references=refs_bleu)\n",
        "print(\"BLEU:\", bleu_result)\n",
        "\n",
        "# Perplexity = exp(eval_loss)\n",
        "eval_metrics = trainer.evaluate()\n",
        "eval_loss = eval_metrics.get(\"eval_loss\", float(\"nan\"))\n",
        "perplexity = np.exp(eval_loss) if isinstance(eval_loss, (int, float)) else float(\"nan\")\n",
        "print(\"Eval loss:\", eval_loss, \"| Perplexity:\", perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Experiment table (fill with your runs)\n",
        "\n",
        "After each training run, copy **Val loss**, **ROUGE-L**, **BLEU**, **Training time**, and **GPU memory** from the cell outputs above into the table. Document at least 2â€“3 experiments (e.g. different LR, epochs, or LoRA rank) for the report.\n",
        "\n",
        "| Exp | LR | Batch | Epochs | LoRA r | Val loss | ROUGE-L | BLEU | Time (min) | GPU mem (GB) | Notes |\n",
        "|-----|-----|-------|--------|--------|----------|---------|------|------------|--------------|------|\n",
        "| 1   | 5e-5 | 2 (acc 4) | 2 | 8 | â€” | â€” | â€” | â€” | â€” | Default |\n",
        "| 2   | 1e-4 | 2 (acc 4) | 2 | 8 | â€” | â€” | â€” | â€” | â€” | Higher LR |\n",
        "| 3   | 5e-5 | 4 (acc 2) | 3 | 16 | â€” | â€” | â€” | â€” | â€” | Larger LoRA, more epochs |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8b. Base vs fine-tuned comparison (for report and demo)\n",
        "\n",
        "The assignment requires comparing the **base pre-trained model** with the **fine-tuned** model. Run the cell below to get responses from both on the same questions. Use this output in your report and demo video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare base (no LoRA) vs fine-tuned. If OOM, set NUM_COMPARE = 2.\n",
        "NUM_COMPARE = 5\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "if USE_4BIT:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        ),\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "else:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID, device_map=\"auto\", trust_remote_code=True, torch_dtype=compute_dtype\n",
        "    )\n",
        "base_model.eval()\n",
        "\n",
        "compare_instructions = [ds_split[\"test\"][i][\"instruction\"] for i in range(min(NUM_COMPARE, len(ds_split[\"test\"])))]\n",
        "compare_references = [ds_split[\"test\"][i][\"response\"] for i in range(min(NUM_COMPARE, len(ds_split[\"test\"])))]\n",
        "base_responses = [generate_response(base_model, tokenizer, inst) for inst in compare_instructions]\n",
        "finetuned_responses = list(predictions[:NUM_COMPARE])\n",
        "\n",
        "del base_model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BASE vs FINE-TUNED (use in report and demo video)\")\n",
        "print(\"=\" * 80)\n",
        "for i in range(len(compare_instructions)):\n",
        "    q = compare_instructions[i].split(\"Question:\")[-1].strip()[:80]\n",
        "    print(f\"\\n--- Example {i+1} ---\\nQuestion: {q}...\")\n",
        "    print(f\"Reference:   {compare_references[i][:180]}...\")\n",
        "    print(f\"Base:        {base_responses[i][:180]}...\")\n",
        "    print(f\"Fine-tuned:  {finetuned_responses[i][:180]}...\")\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat(user_input, history=None):\n",
        "    if history is None:\n",
        "        history = []\n",
        "    instruction = f\"You are an agriculture assistant. Answer the following question.\\n\\nQuestion: {user_input}\"\n",
        "    reply = generate_response(model, tokenizer, instruction)\n",
        "    history.append((user_input, reply))\n",
        "    return history, history\n",
        "\n",
        "with gr.Blocks(title=\"Agriculture Assistant\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"## ðŸŒ¾ Agriculture QA Assistant\\n\\n**Instructions:** Type your agriculture-related question in the box below and click **Submit** (or press Enter). The fine-tuned model will answer. Use **Clear** to start a new conversation. For your demo video, try in-domain questions (e.g. pests, crops, soil) and optionally an out-of-domain question to show the model stays on topic.\\n\\nModel fine-tuned on [sowmya14/agriculture_QA](https://huggingface.co/datasets/sowmya14/agriculture_QA).\"\"\")\n",
        "    chatbot = gr.Chatbot(label=\"Chat\")\n",
        "    msg = gr.Textbox(placeholder=\"e.g. What are the best practices for soil preparation?\", label=\"Your question\")\n",
        "    submit = gr.Button(\"Submit\")\n",
        "    clear = gr.Button(\"Clear\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    def submit_fn(msg, history):\n",
        "        if not msg.strip():\n",
        "            return history, history\n",
        "        _, new_history = chat(msg, history)\n",
        "        return new_history, new_history\n",
        "\n",
        "    submit.click(submit_fn, [msg, state], [chatbot, state])\n",
        "    clear.click(lambda: ([], []), None, [chatbot, state])\n",
        "    msg.submit(submit_fn, [msg, state], [chatbot, state])\n",
        "\n",
        "# In Colab the app appears below; share=True gives a public URL for your demo video.\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
